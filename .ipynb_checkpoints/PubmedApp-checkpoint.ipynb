{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:33507/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jun/2017 10:16:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jun/2017 10:16:14] \"GET /static/style.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jun/2017 10:16:14] \"GET /static/loading.gif HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Retrieved:0.7766218185424805\n",
      "Gathered PMID Summary for counts:1.5274169445037842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jun/2017 10:16:20] \"POST /search HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jun/2017 10:16:20] \"GET /static/styleview.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jun/2017 10:16:24] \"POST /geo HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PMIDs\n",
      "1610 Articles Found:0.6602351665496826\n",
      "Getting Cited PMIDs\n",
      "References Retrieved:1.3681368827819824\n",
      "Getting Info of PMIDs\n",
      "Info Retrieved:0.5499248504638672\n",
      "Performing sparse cosine similarity\n",
      "Cosine similarity time:0.3741469383239746\n",
      "Performing TSNE\n",
      "636\n",
      "TSNE calc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jun/2017 10:16:34] \"POST /similarity HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE time:3.94291090965271\n",
      "Total Time:7.315470218658447\n"
     ]
    }
   ],
   "source": [
    "###Main page: PubmedApp\n",
    "#mainspot\n",
    "\n",
    "\n",
    "#import string as st\n",
    "from flask import Flask, render_template, request, redirect\n",
    "from bokeh.embed import components\n",
    "#import PubDatePlotting as pdp\n",
    "#import StateGraph as sg\n",
    "#import SimilarityPlot as smp\n",
    "\n",
    "# output_notebook()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.vars = {}\n",
    "\n",
    "#setting up navigation info\n",
    "\n",
    "app.nav_id = {'counts':'counts','geo':'geo','similarity':'similarity'}\n",
    "app.nav_name = {'counts':'Raw Counts','geo':'Geographic','similarity':'Visualize Article Similarity'}\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    # nquestions=app_lulu.nquestions\n",
    "    if request.method == 'GET':\n",
    "        return render_template('pubsearch.html')\n",
    "    else:\n",
    "        \n",
    "        app.vars == {}\n",
    "\n",
    "#         app.vars['cancertype'] = request.form['cancertype']\n",
    "\n",
    "#         p1 = pdp.pubByDate(app.vars['cancertype'], False)\n",
    "#         p2 = pdp.pubByDate(app.vars['cancertype'], True)\n",
    "#         p3, p4 = stateGraph(app.vars['cancertype'])\n",
    "#         plots = {'p1': p1, 'p2': p2, 'p3': p3, 'p4': p4}\n",
    "#         script, div = components(plots)\n",
    "\n",
    "        #return render_template('pubsearch.html', script=script, div=div, ctype=app.vars['cancertype'])\n",
    "        return render_template('pubsearch.html')\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/search', methods=['GET', 'POST'])\n",
    "def searchView():\n",
    "    if request.method == 'GET':\n",
    "        app.vars = {}\n",
    "        return render_template('pubsearch.html')\n",
    "    else:\n",
    "        app.vars = {}\n",
    "        app.vars['searchStr'] = request.form['searchterm']\n",
    "        return countsView()\n",
    "\n",
    "\n",
    "        \n",
    "@app.route('/counts', methods=['POST'])\n",
    "def countsView():\n",
    "    app.curpage = \"counts\"\n",
    "    if 'countsData' not in app.vars:\n",
    "        \n",
    "        #generate random 20 char string as identifier\n",
    "        #rstr = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n",
    "        #app.vars['countsData'] = rstr\n",
    "        app.vars['countsData'] = True\n",
    "        \n",
    "#         while True:\n",
    "#             try:\n",
    "#                 app.yearplot = yearGraph(app.vars['searchStr'])\n",
    "#                 break\n",
    "#             except:\n",
    "#                 print(\"Error getting year data\")\n",
    "        app.yearplot = yearGraph(app.vars['searchStr'],1975,2017)\n",
    "        #app.yearplot = pdp.yearGraph(app.vars['searchStr'],1975,2017)\n",
    "        plots = {'yearplot': app.yearplot}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        plots = {'yearplot': app.yearplot}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "@app.route('/geo', methods=['POST'])\n",
    "def geoView():\n",
    "    app.curpage = \"geo\"\n",
    "    if 'geoData' not in app.vars:\n",
    "        app.vars['geoData'] = True\n",
    "        \n",
    "        #connect to Pubmed and try to get data, if fail restart\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 app.stateplot, app.stateplotnorm = stateGraph(app.vars['searchStr'])\n",
    "#                 break\n",
    "#             except:\n",
    "#                 print(\"Error getting state data\")\n",
    "                \n",
    "        app.stateplot = stateGraph(app.vars['searchStr'],\"1975/01/01\",\"2016/12/31\")\n",
    "        #app.stateplot = sg.stateGraph(app.vars['searchStr'],\"1975/01/01\",\"2016/12/31\")\n",
    "        plots = {'stateplot': app.stateplot}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        \n",
    "        plots = {'stateplot': app.stateplot}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "@app.route('/similarity', methods=['POST'])\n",
    "def similarityView():\n",
    "    app.curpage = \"similarity\"\n",
    "    if 'similarity' not in app.vars:\n",
    "        app.vars['similarity'] = True\n",
    "        \n",
    "        \n",
    "        app.simplot = similarityGraph(app.vars['searchStr'], '1975', '2017')\n",
    "        #app.simplot = smp.similarityGraph(app.vars['searchStr'], '1975', '2017')\n",
    "        script, div = components({'column_div': app.simplot})\n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        script, div = components({'column_div': app.simplot})\n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=33507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Similarity plotting of search\n",
    "import time\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from math import ceil\n",
    "from urllib.parse import quote\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import column, layout\n",
    "from bokeh.models import HoverTool, CustomJS, OpenURL, TapTool, Range1d\n",
    "from bokeh.models.widgets import TextInput, Button\n",
    "#output_notebook()\n",
    "\n",
    "#Articles that a given article cites\n",
    "#https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&id=24923681\n",
    "#add to end of elink post: &query_key=<key>&WebEnv=<webenv string>\n",
    "#Parks Stone Pubmed ID 24923681\n",
    "\n",
    "# UID list. Either a single UID or a comma-delimited list of UIDs may be provided.\n",
    "# All of the UIDs must be from the database specified by dbfrom. \n",
    "# There is no set maximum for the number of UIDs that can be passed to ELink, \n",
    "# but if more than about 200 UIDs are to be provided, the request should be made using the HTTP POST method.\n",
    "\n",
    "#similarity score for comparing sets, ie cited articles\n",
    "#see http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/\n",
    "# \n",
    "\n",
    "# def returnJaccard(cids):\n",
    "#     lenList = len(cids)\n",
    "#     jarr = np.zeros([lenList,lenList])\n",
    "#     for ix in range(lenList):\n",
    "#         for jx in range(lenList):\n",
    "#             if(ix>jx):\n",
    "#                 jc = jaccard(cids[ix],cids[jx])\n",
    "#                 jarr[ix][jx] = jc\n",
    "#                 jarr[jx][ix] = jc\n",
    "#     print(jarr[:5])\n",
    "#     return jarr\n",
    "\n",
    "# def jaccard(x,y):\n",
    "\n",
    "#     intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "#     union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "#     return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "### Get ids (PMID) for papers in search\n",
    "def PMIDsFromSearch(s,sy,ey):\n",
    "    pre = time.time()\n",
    "    \n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+s+\"&mindate=\"+sy+\"/01/01&maxdate=\"+ey+\"/12/31&usehistory=y&retmode=json\"\n",
    "    #pre = time.time()\n",
    "    search_r = requests.post(search_url)\n",
    "    #print(\"Web Env retrieved:\"+str(time.time()-pre))\n",
    "    \n",
    "    search_data = search_r.json()\n",
    "    web_env = search_data['esearchresult']['webenv']\n",
    "    query_key = search_data[\"esearchresult\"]['querykey']\n",
    "    total_count = search_data[\"esearchresult\"]['count']\n",
    "    \n",
    "    \n",
    "    maxIds = 100000 #maximum defined by pubmed\n",
    "    rids = []\n",
    "    #loop over all articles and grab set cited papers (see cited_rosetta for paper structure)\n",
    "    for i in range(ceil(int(total_count)/maxIds)):\n",
    "        \n",
    "        id_url = (\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+s+\n",
    "                                \"&mindate=\"+sy+\"/01/01&maxdate=\"+ey+\"/12/31&usehistory=y&retmode=json&retstart=\"+\n",
    "                                str(i*maxIds)+\"&retmax=\"+str(i*maxIds+maxIds-1))\n",
    "        id_r = requests.get(id_url)\n",
    "        id_data = id_r.json()\n",
    "        id_list = id_data[\"esearchresult\"]['idlist']\n",
    "        for tid in id_list:\n",
    "            rids.append(int(tid))\n",
    "    \n",
    "    #deletes duplicate entries\n",
    "    rids = list(set(rids))\n",
    "    print(str(len(rids))+\" Articles Found:\"+str(time.time()-pre))\n",
    "    \n",
    "    return rids\n",
    "\n",
    "\n",
    "#fetch async requests\n",
    "async def fetchCited(url,postdata, session):\n",
    "    async with session.post(url,data=postdata) as response:\n",
    "        return await response.text()\n",
    "\n",
    "###get subsets of ids in async fashion############\n",
    "async def runCited(url,ids):\n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    async with ClientSession() as session:\n",
    "        for ix in ids:\n",
    "            post_vars = {\"dbfrom\":\"pubmed\", \"linkname\":\"pubmed_pubmed_refs\"}\n",
    "            post_vars[\"id\"] = ix;\n",
    "            task = asyncio.ensure_future(fetchCited(url, post_vars, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # you now have all response bodies in this variable\n",
    "        #responses can be converted to json, originally were strings\n",
    "        #print(json.loads(responses[0]))\n",
    "    return responses\n",
    "\n",
    "def getCitedFromPMIDs(rids):\n",
    "    ids = []\n",
    "    cids = []\n",
    "    \n",
    "    \n",
    "    cited_post = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\"\n",
    "    #example of get fetch \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&id=\n",
    "    \n",
    "    #nreqs is number of ids to put into each async op, eg 1200 ids will result in 12 async ops, currently set to 100 requests total\n",
    "    nreqs = ceil(len(rids)/100)\n",
    "    \n",
    "    if(nreqs>1):\n",
    "        rids = [rids[i*100:i*100+100] for i in range(0, nreqs)]\n",
    "    \n",
    "    #timer\n",
    "    pre = time.time()\n",
    "    \n",
    "    #get the async references, connecs can fail, try 10 times before error\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(runCited(cited_post,rids))\n",
    "    res = loop.run_until_complete(future)\n",
    "    \n",
    "    print(\"References Retrieved:\"+str(time.time()-pre))\n",
    "    \n",
    "    ##Only adds those articles that have citations listed in LinkSetDb (ie articles that are listed in PMC)c\n",
    "    for r in res:\n",
    "        #print(r)\n",
    "        croot = ET.fromstring(r)\n",
    "        for linkset in croot.iter('LinkSet'):\n",
    "            tlinks = []\n",
    "            ttid = linkset.find('IdList')[0]\n",
    "            if(len(list(linkset))>2):\n",
    "                for link in linkset.find('LinkSetDb').iter('Link'):\n",
    "                    tlid = link[0]\n",
    "                    tlinks.append(tlid.text)\n",
    "                cids.append(tlinks)\n",
    "                ids.append(int(ttid.text))\n",
    "    return ids, cids\n",
    "\n",
    "# def getCitedFromPMIDXML(r):\n",
    "#     ids = []\n",
    "#     cids = []\n",
    "\n",
    "#     cited_post = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\"\n",
    "#     post_vars = {\"dbfrom\":\"pubmed\", \"linkname\":\"pubmed_pubmed_refs\",\"id\":[]}\n",
    "\n",
    "#     #loop over all articles and grab set cited papers (see cited_rosetta for paper structure)\n",
    "#     for tid in r[0][1].iter('Id'):\n",
    "#         post_vars['id'].append(tid.text)\n",
    "    \n",
    "#     pre = time.time()\n",
    "#     cited_fetch = requests.post(cited_post, data=post_vars)\n",
    "#     print(\"References Retrieved:\"+str(time.time()-pre))\n",
    "#     croot = ET.fromstring(cited_fetch.text)\n",
    "    \n",
    "#     for linkset in croot.iter('LinkSet'):\n",
    "#         tlinks = []\n",
    "#         ttid = linkset.find('IdList')[0]\n",
    "#         if(len(list(linkset))>2):\n",
    "#             for link in linkset.find('LinkSetDb').iter('Link'):\n",
    "#                 tlid = link[0]\n",
    "#                 tlinks.append(tlid.text)\n",
    "#             cids.append(tlinks)\n",
    "#             ids.append(int(ttid.text))\n",
    "#     return ids, cids\n",
    "\n",
    "\n",
    "\n",
    "########Grabs all ids summary to get the title, authors, pubdate for each PMID\n",
    "def getPMIDInfo(ids):\n",
    "    #use full journal name\n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "    journals = []\n",
    "    pmccites = []\n",
    "    \n",
    "    #https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&version=2.0&id=27656642,24923681\n",
    "    info_base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    info_data = {\"db\":\"pubmed\", \"version\":\"2.0\"}\n",
    "    info_str = \"\"\n",
    "    info_post = info_base\n",
    "    \n",
    "    #loop over all PMIDs\n",
    "    for tid in ids:\n",
    "        info_str += str(tid)+\",\"\n",
    "    \n",
    "    info_str = info_str[:-1]\n",
    "    info_data[\"id\"] = info_str\n",
    "    #print(info_str)\n",
    "    \n",
    "    pre = time.time()\n",
    "    info_fetch = requests.post(info_post, data=info_data)\n",
    "    print(\"Info Retrieved:\"+str(time.time()-pre))\n",
    "    \n",
    "    croot = ET.fromstring(info_fetch.text)\n",
    "\n",
    "    for doc in croot[0].iter('DocumentSummary'):\n",
    "        titles.append(doc.find('Title').text)\n",
    "        dates.append(doc.find('PubDate').text)\n",
    "        journals.append(doc.find('FullJournalName').text)\n",
    "        pmccites.append(doc.find('PmcRefCount').text)\n",
    "        \n",
    "        tempAuths = []\n",
    "        for auth in doc.find('Authors').iter('Author'):\n",
    "            tempAuths.append(auth.find('Name').text)\n",
    "        authors.append(tempAuths)\n",
    "    \n",
    "\n",
    "    return titles, dates, authors, journals, pmccites\n",
    "\n",
    "########CHANGE CODE BELOW need to grab all ids summary to get the title, authors, pubdate for each\n",
    "# def getPMIDInfo(ids):\n",
    "#     #use full journal name\n",
    "#     titles = []\n",
    "#     dates = []\n",
    "#     authors = []\n",
    "#     journals = []\n",
    "    \n",
    "#     #https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&version=2.0&id=27656642,24923681\n",
    "#     info_base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&version=2.0&id=\"\n",
    "#     info_post = info_base\n",
    "    \n",
    "#     #loop over all PMIDs\n",
    "#     for tid in ids:\n",
    "#         if((len(info_post)+len(str(tid))+4)>2083):\n",
    "            \n",
    "#             #print(\"fetching\")\n",
    "#             info_post = info_post[:-1]\n",
    "#             info_fetch = requests.post(info_post)\n",
    "#             info_post = info_base+str(tid)+\",\"\n",
    "#             croot = ET.fromstring(info_fetch.text)\n",
    "\n",
    "#             #get data from croot\n",
    "#             for doc in croot[0].iter('DocumentSummary'):\n",
    "#                 titles.append(doc.find('Title').text)\n",
    "#                 dates.append(doc.find('PubDate').text)\n",
    "#                 journals.append(doc.find('FullJournalName').text)\n",
    "#                 aus = \"\"\n",
    "#                 for auth in doc.find('Authors').iter('Author'):\n",
    "#                     aus += auth.find('Name').text+\", \"\n",
    "#                 authors.append(aus[:-2])\n",
    "                \n",
    "#         else:\n",
    "#             info_post += str(tid)+\",\"\n",
    "\n",
    "#     #for grabbing last set of Links\n",
    "    \n",
    "#     info_post = info_post[:-1]\n",
    "#     info_fetch = requests.post(info_post)\n",
    "#     croot = ET.fromstring(info_fetch.text)\n",
    "\n",
    "#     for doc in croot[0].iter('DocumentSummary'):\n",
    "#         titles.append(doc.find('Title').text)\n",
    "#         dates.append(doc.find('PubDate').text)\n",
    "#         journals.append(doc.find('FullJournalName').text)\n",
    "#         aus = \"\"\n",
    "#         for auth in doc.find('Authors').iter('Author'):\n",
    "#             aus += auth.find('Name').text+\", \"\n",
    "#         authors.append(aus[:-2])\n",
    "#     return titles, dates, authors, journals\n",
    "\n",
    "\n",
    "def returnCosine(cids):\n",
    "    column = np.hstack(cids).astype(np.float)\n",
    "    row = np.hstack([np.ones(len(arr))*i for i, arr in enumerate(cids)]).astype(np.float)\n",
    "    vals = np.empty(column.size)\n",
    "    vals.fill(1)\n",
    "    mat = sp.csc_matrix((vals, (row, column)))\n",
    "    \n",
    "    \n",
    "    jarr = cosine_similarity(mat)\n",
    "    #print(jarr[:5])\n",
    "    return jarr\n",
    "\n",
    "\n",
    "def calcTSNE(X):\n",
    "    print(len(X))\n",
    "# plot didn't look very good with spectral embedding\n",
    "#     if(len(X)>3000):\n",
    "#         print(\"SVD calc\")\n",
    "#         #X = jarr\n",
    "#         pre = time.time()\n",
    "#         svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "#         Xr = svd.fit_transform(X)\n",
    "#         print(\"SVD runtime:\"+str(time.time()-pre))\n",
    "        \n",
    "#         print(\"Spectral Embedding\")\n",
    "#         #X = jarr\n",
    "#         pre = time.time()\n",
    "#         se = SpectralEmbedding(n_components=2, random_state=0, eigen_solver=\"arpack\")\n",
    "#         Y = se.fit_transform(Xr)\n",
    "#         print(\"Spectral Embedding runtime:\"+str(time.time()-pre))\n",
    "        \n",
    "    if(len(X)>1000):\n",
    "        print(\"SVD calc\")\n",
    "        #X = jarr\n",
    "        pre = time.time()\n",
    "        svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "        Xr = svd.fit_transform(X)\n",
    "        print(\"SVD runtime:\"+str(time.time()-pre))\n",
    "\n",
    "        print(\"TSNE calc\")\n",
    "        pre = time.time()\n",
    "        tsn = TSNE(n_components=2, random_state=0)\n",
    "        Y = tsn.fit_transform(Xr);\n",
    "        print(\"TSNE time:\"+str(time.time()-pre))\n",
    "    else:\n",
    "        print(\"TSNE calc\")\n",
    "        tsn = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "        pre = time.time()\n",
    "        Y = tsn.fit_transform(X);\n",
    "        print(\"TSNE time:\"+str(time.time()-pre))\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# print(\"xmax \"+str(np.amax(Y[:,0])))\n",
    "# print(\"xmin \"+str(np.amin(Y[:,0])))\n",
    "# print(\"ymax \"+str(np.amax(Y[:,1])))\n",
    "# print(\"ymin \"+str(np.amin(Y[:,1])))\n",
    "def similarityGraph(si, sy, ey):\n",
    "    pres = time.time()\n",
    "    ss = quote(si)\n",
    "    \n",
    "    print(\"Searching for PMIDs\")\n",
    "    rids = PMIDsFromSearch(ss, sy, ey)\n",
    "    \n",
    "    print(\"Getting Cited PMIDs\")\n",
    "    ids, cids = getCitedFromPMIDs(rids)\n",
    "    \n",
    "    print(\"Getting Info of PMIDs\")\n",
    "    titles, dates, authors, journals, pmccites = getPMIDInfo(ids)\n",
    "    \n",
    "    print(\"Performing sparse cosine similarity\")\n",
    "    pre = time.time()\n",
    "    carr = returnCosine(cids)\n",
    "    print(\"Cosine similarity time:\"+str(time.time()-pre))\n",
    "\n",
    "    print(\"Performing TSNE\")\n",
    "    Y = calcTSNE(carr)\n",
    "    print(\"Total Time:\"+str(time.time()-pres))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #convert authors list of lists to list of strings for display\n",
    "    authors_str = []\n",
    "    for auths in authors:\n",
    "        authors_str.append(\", \".join(auths))\n",
    "    \n",
    "    \n",
    "    colors = ['blue']*len(ids)\n",
    "    alphas = [1]*len(ids)\n",
    "    source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=Y[:,0],\n",
    "                y=Y[:,1],\n",
    "                PMID=ids,\n",
    "                titles=titles,\n",
    "                authors=authors_str,\n",
    "                journals=journals,\n",
    "                dates=dates,\n",
    "                colors=colors,\n",
    "                alphas=alphas,\n",
    "                pmccites=pmccites\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #####max-width IS IMPORTANT FOR PROPER WRAPPING OF TEXT\n",
    "    hover = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@titles</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@authors</span>\n",
    "                <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-style: italic;\">@journals, @dates</span>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 10px;\">PMID</span>\n",
    "                    <span style=\"font-size: 10px; color: #696;\">@PMID</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 10px;\">PMC Citations</span>\n",
    "                    <span style=\"font-size: 10px; color: #696;\">@pmccites</span>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    resetCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var titles = data['titles']\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            data.colors[i]='blue'\n",
    "            data.alphas[i]= 1\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "    textCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var value = cb_obj.get('value')\n",
    "        var words = value.split(\" \")\n",
    "        for (i=0; i < data.titles.length; i++) {\n",
    "            data.alphas[i]= 0.3\n",
    "        }\n",
    "        for (i=0; i < data.titles.length; i++) {\n",
    "            for(j=0; j < words.length; j++){\n",
    "                if (data.titles[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1) { \n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange'\n",
    "                        data.alphas[i]= 1\n",
    "                    }\n",
    "                }else if(data.authors[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange'\n",
    "                        data.alphas[i]= 1\n",
    "                    }\n",
    "                }else if(data.journals[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange'\n",
    "                        data.alphas[i]= 1\n",
    "                    }\n",
    "                }else{\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    TOOLS = 'pan,wheel_zoom,tap,reset'\n",
    "    p = figure(plot_width=900, plot_height=600, title=\"'\"+si+\"' tSNE similarity\", tools=[TOOLS,hover], active_scroll='wheel_zoom')\n",
    "\n",
    "    p.circle('x', 'y',fill_color='colors', fill_alpha='alphas', size=12, source=source)\n",
    "\n",
    "    #formatting plot\n",
    "    p.xaxis.axis_label = \"Hover to view publication info, Click to open Pubmed link\"\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'\n",
    "    left, right, bottom, top = np.amin(Y[:,0])*1.1, np.amax(Y[:,0])*1.1, np.amin(Y[:,1])*1.1, np.amax(Y[:,1])*1.1\n",
    "    p.x_range=Range1d(left, right)\n",
    "    p.y_range=Range1d(bottom, top)\n",
    "\n",
    "\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pubmed/@PMID\"\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "\n",
    "\n",
    "    word_input = TextInput(title=\"Search for term(s) in graph\", placeholder=\"Enter term to highlight\", callback=textCallback)\n",
    "    reset = Button(label=\"Clear Highlighting\", callback=resetCallback, width=150)\n",
    "\n",
    "    lt = layout([[word_input],[reset], [p]])\n",
    "\n",
    "    return lt\n",
    "\n",
    "# ss = \"telomerase rna\"\n",
    "# syear = \"1975\"\n",
    "# eyear = \"2017\"   \n",
    "# lt = similarityGraph(ss,syear,eyear)\n",
    "# show(lt)\n",
    "\n",
    "# print(jarr[:5])\n",
    "# print(ids[:5])\n",
    "# print(cids[:5])\n",
    "# print(titles[:5])\n",
    "# print(authors[:5])\n",
    "# print(dates[:5])\n",
    "# print(journals[:5])\n",
    "\n",
    "\n",
    "# cited_fetch = requests.post(cited_post)\n",
    "# cited_xml += cited_fetch.text\n",
    "# f = open(\"cited_\"+ss+\".xml\", 'w')\n",
    "# f.write(cited_xml)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############ASYNCIO PubByDate\n",
    "\n",
    "import asyncio\n",
    "import urllib\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "from urllib.parse import quote\n",
    "from collections import Counter\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn, Div\n",
    "from bokeh.models import CustomJS\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import layout\n",
    "#from SimilarityPlot import getPMIDInfo\n",
    "\n",
    "async def fetchYears(url, session):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "###add years correlating with responses############\n",
    "async def runYears(yrs, ss):\n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    async with ClientSession() as session:\n",
    "        for y in yrs:\n",
    "            tu = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+ss+\"&mindate=\"+str(y)+\"/01/01&maxdate=\"+str(y)+\"/12/31&usehistory=y&retmode=json&retmax=100000\"\n",
    "            task = asyncio.ensure_future(fetchYears(tu, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # you now have all response bodies in this variable\n",
    "        #responses can be converted to json, originally were strings\n",
    "        #print(json.loads(responses[0]))\n",
    "    return responses\n",
    "\n",
    "# gets data from each state while string : ss=searchstring\n",
    "def getYears(ss,yrs):\n",
    "    \n",
    "    yearcounts = []\n",
    "    #start threads and create queue of URLs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(runYears(yrs,ss))\n",
    "    res = loop.run_until_complete(future)\n",
    "    #print(res)\n",
    "    rids =[]\n",
    "    for idx, y in enumerate(yrs):\n",
    "        search_data = json.loads(res[idx])\n",
    "        #webenv = search_data[\"esearchresult\"]['webenv']\n",
    "        total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "        yearcounts.append(total_records) \n",
    "        \n",
    "        id_list = search_data[\"esearchresult\"]['idlist']\n",
    "        for tid in id_list:\n",
    "            rids.append(int(tid))\n",
    "\n",
    "    return yearcounts, rids\n",
    "        \n",
    "\n",
    "def yearGraph(si,sy,ey):\n",
    "    years = list(range(sy, ey))\n",
    "    \n",
    "    ss = quote(si)\n",
    "    p = figure(title=\"Articles containing: \"+si, plot_width=600, plot_height=300, x_axis_label=\"Year\", y_axis_label=\"Total Articles\")\n",
    "\n",
    "    ##FOR SEARCHING YOU WILL NEED TO ESCAPE SPACES AND SPECIAL CHARS\n",
    "    yearcounts, ids = getYears(ss,years)\n",
    "    pre = time.time()\n",
    "    titles, dates, authors, journals, pmccites = getPMIDInfo(ids)\n",
    "    print(\"Gathered PMID Summary for counts:\"+str(time.time()-pre))\n",
    "    \n",
    "    p.line(years, yearcounts, color='blue')\n",
    "    \n",
    "    #Parse out statisitcs about returned data\n",
    "    authors_str = []\n",
    "    for auths in authors:\n",
    "        authors_str.append(\", \".join(auths))\n",
    "    pmidPD = pd.DataFrame(data={'titles':titles,'dates':dates,'authors':authors,'authors_str':authors_str,'journals':journals,'pmccites':list(map(int,pmccites))})\n",
    "    \n",
    "    #print(pmidPD[:5])\n",
    "    topPmids = pmidPD.nlargest(10,'pmccites')\n",
    "    #####ADD CODE TO DISPLAY TABLE NEXT TO PLOT, determine how good a proxy pmccites is for real number of citation\n",
    "    #print(topPmids)\n",
    "   \n",
    "    total_authors = Counter(auth for auths in pmidPD['authors'] for auth in auths)\n",
    "    total_authors_norm = Counter()\n",
    "    for idx, auths in enumerate(pmidPD['authors']):\n",
    "        for auth in auths:\n",
    "            total_authors_norm[auth] += pmidPD['pmccites'][idx]\n",
    "            \n",
    "    total_journals = Counter(jour for jour in pmidPD['journals'])\n",
    "    \n",
    "    #get data ready for display in bokeh\n",
    "    #display nent number of entries\n",
    "    nent = 100\n",
    "    \n",
    "    topauths = total_authors.most_common(nent)\n",
    "    topauths_norm = total_authors_norm.most_common(nent)\n",
    "    topjournals = total_journals.most_common(nent)\n",
    "    \n",
    "    #for displaying authors based on citations and papers\n",
    "    paper_data = dict(\n",
    "        numpapers = [n[1] for n in topauths],\n",
    "        auths = [n[0] for n in topauths]\n",
    "    )\n",
    "    cite_data = dict(\n",
    "        numcites = [n[1] for n in topauths_norm],\n",
    "        authscites = [n[0] for n in topauths_norm]\n",
    "    )\n",
    "    journal_data = dict(\n",
    "        numpapers = [n[1] for n in topjournals],\n",
    "        journals = [n[0] for n in topjournals]\n",
    "    )\n",
    "    #print(len(titles),len(dates),len(journals),len(authors_str),len(pmccites))\n",
    "    #print(journals)\n",
    "    pub_data = dict(\n",
    "        titles = titles,\n",
    "        dates = dates,\n",
    "        journals = journals,\n",
    "        authors = authors_str,\n",
    "        pmccites = pmccites\n",
    "    )\n",
    "    \n",
    "    pubview_data = dict(\n",
    "        titles = [\"Title\"],\n",
    "        dates = [\"Date\"],\n",
    "        journals = [\"Journal\"],\n",
    "        authors = [\"Author\"],\n",
    "        pmccites = [\"PMC Citations\"]\n",
    "    )\n",
    "    #publication table that will be populated by \n",
    "    \n",
    "    paper_source = ColumnDataSource(data = paper_data)\n",
    "    cite_source = ColumnDataSource(data = cite_data)\n",
    "    journal_source = ColumnDataSource(data = journal_data)\n",
    "    \n",
    "    pub_source = ColumnDataSource(pub_data)\n",
    "    pubview_source = ColumnDataSource(pubview_data)\n",
    "\n",
    "    paper_columns = [\n",
    "            TableColumn(field=\"numpapers\", title=\"# Papers\",width=100),\n",
    "            TableColumn(field=\"auths\", title=\"Author\",width=150),\n",
    "        ]\n",
    "    cite_columns = [\n",
    "            TableColumn(field=\"numcites\", title=\"# PMC Citations\",width=130),\n",
    "            TableColumn(field=\"authscites\", title=\"Author\",width=150),\n",
    "        ]\n",
    "    \n",
    "    journal_columns = [\n",
    "            TableColumn(field=\"numpapers\", title=\"# Papers\",width=80),\n",
    "            TableColumn(field=\"journals\", title=\"Journal\", width=220),\n",
    "        ]\n",
    "    \n",
    "    pubview_columns = [\n",
    "            TableColumn(field=\"titles\", title=\"Article Title\"),\n",
    "            TableColumn(field=\"authors\", title=\"Authors\"),\n",
    "            TableColumn(field=\"journals\", title=\"Journal\"),\n",
    "            TableColumn(field=\"dates\", title=\"Date\",width = 80),\n",
    "            TableColumn(field=\"pmccites\", title=\"PMC Citations\", width =80),\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    paper_table = DataTable(source=paper_source, columns=paper_columns, width=250, height=210)\n",
    "    cite_table = DataTable(source=cite_source, columns=cite_columns, width=280, height=210)\n",
    "    journal_table = DataTable(source=journal_source, columns=journal_columns, width=300, height=210)\n",
    "    \n",
    "    tit1 = Div(text=\"<h1>Common Authors and Journals</h1>\\n<h5>(Select author(s) or journal(s) to display related publications)</h5>\",width=930)\n",
    "    tit2 = Div(text=\"<h1>Related Publications</h1>\\n<h5>(click publication to visit pubmed entry)</h5>\", width=930)\n",
    "    \n",
    "    spacer = figure(plot_width=50, plot_height=210, logo = None, toolbar_location = None, outline_line_color = None)\n",
    "    spacer2 = figure(plot_width=50, plot_height=210, logo = None, toolbar_location = None, outline_line_color = None)\n",
    "    \n",
    "    #create table updated by clicks in above tables, for viewing related publications\n",
    "    \n",
    "    pubview_table = DataTable(source=pubview_source, columns=pubview_columns, width=930, height=400)\n",
    "    \n",
    "    # callbacks for each of the table selections\n",
    "    \n",
    "    \n",
    "    #JScallback for setting up publications table\n",
    "    #curspot\n",
    "    paper_source.callback = CustomJS(args=dict(paper_source = paper_source, pub_source = pub_source, pubview_table = pubview_table), code=\"\"\"\n",
    "        var authordata = paper_source.selected[\"1d\"].indices\n",
    "        var author = 'test'\n",
    "        var count = 0\n",
    "        var s1 = paper_source.get('data');\n",
    "        var d1 = pub_source.get('data');\n",
    "        var d2 = pubview_table.get('source').get('data');\n",
    "        d2.index = []\n",
    "        d2.authors = []\n",
    "        d2.titles = []\n",
    "        d2.journals = []\n",
    "        d2.dates = []\n",
    "        d2.pmccites = []\n",
    "        for(j = 0; j < d1.authors.length; j++){\n",
    "            for(k = 0; k < authordata.length; k++){\n",
    "                if (d1.authors[j].toLowerCase().indexOf(s1.auths[authordata[k]].toLowerCase()) !== -1) { \n",
    "                    d2.index.push(count)\n",
    "                    d2.authors.push(d1.authors[j])\n",
    "                    d2.titles.push(d1.titles[j])\n",
    "                    d2.journals.push(d1.journals[j])\n",
    "                    d2.dates.push(d1.dates[j])\n",
    "                    d2.pmccites.push(parseInt(d1.pmccites[j]))\n",
    "                    count += 1\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        console.log(d2)\n",
    "        pubview_table.trigger('change');\n",
    "        \"\"\")\n",
    "    \n",
    "    cite_source.callback = CustomJS(args=dict(cite_source = cite_source, pub_source = pub_source, pubview_table = pubview_table), code=\"\"\"\n",
    "        var authordata = cite_source.selected[\"1d\"].indices\n",
    "        var author = 'test'\n",
    "        var count = 0\n",
    "        var s1 = cite_source.get('data');\n",
    "        var d1 = pub_source.get('data');\n",
    "        var d2 = pubview_table.get('source').get('data');\n",
    "        d2.index = []\n",
    "        d2.authors = []\n",
    "        d2.titles = []\n",
    "        d2.journals = []\n",
    "        d2.dates = []\n",
    "        d2.pmccites = []\n",
    "        for(j = 0; j < d1.authors.length; j++){\n",
    "            for(k = 0; k < authordata.length; k++){\n",
    "                if (d1.authors[j].toLowerCase().indexOf(s1.authscites[authordata[k]].toLowerCase()) !== -1) { \n",
    "                    d2.index.push(count)\n",
    "                    d2.authors.push(d1.authors[j])\n",
    "                    d2.titles.push(d1.titles[j])\n",
    "                    d2.journals.push(d1.journals[j])\n",
    "                    d2.dates.push(d1.dates[j])\n",
    "                    d2.pmccites.push(parseInt(d1.pmccites[j]))\n",
    "                    count += 1\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        console.log(d2)\n",
    "        pubview_table.trigger('change');\n",
    "        \"\"\")\n",
    "    journal_source.callback = CustomJS(args=dict(journal_source = journal_source, pub_source = pub_source, pubview_table = pubview_table), code=\"\"\"\n",
    "        var journaldata = journal_source.selected[\"1d\"].indices\n",
    "        var author = 'test'\n",
    "        var count = 0\n",
    "        var s1 = journal_source.get('data');\n",
    "        var d1 = pub_source.get('data');\n",
    "        var d2 = pubview_table.get('source').get('data');\n",
    "        d2.index = []\n",
    "        d2.authors = []\n",
    "        d2.titles = []\n",
    "        d2.journals = []\n",
    "        d2.dates = []\n",
    "        d2.pmccites = []\n",
    "        for(j = 0; j < d1.journals.length; j++){\n",
    "            for(k = 0; k < journaldata.length; k++){\n",
    "                //checks to make sure journal name exists, seems like certain publications have no journal name in pubmed\n",
    "                if(d1.journals[j]){\n",
    "                    if (d1.journals[j].toLowerCase() == s1.journals[journaldata[k]].toLowerCase()) { \n",
    "                        d2.index.push(count)\n",
    "                        d2.authors.push(d1.authors[j])\n",
    "                        d2.titles.push(d1.titles[j])\n",
    "                        d2.journals.push(d1.journals[j])\n",
    "                        d2.dates.push(d1.dates[j])\n",
    "                        d2.pmccites.push(parseInt(d1.pmccites[j]))\n",
    "                        count += 1\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        console.log(d2)\n",
    "        pubview_table.trigger('change');\n",
    "        \"\"\")\n",
    "    \n",
    "    ###### write callbacks for other two tables as well\n",
    "    \n",
    "    \n",
    "    yearslayout = layout([[p],[tit1],[paper_table,spacer,cite_table,spacer2,journal_table],[tit2],[pubview_table]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #show(yearslayout)\n",
    "        \n",
    "#     print(total_authors.most_common(10))\n",
    "#     print(total_authors_norm.most_common(10))\n",
    "#     print(total_authors['Harley CB'], total_authors['Thomson JA'])\n",
    "    \n",
    "\n",
    "    #show(p)\n",
    "    return yearslayout\n",
    "# yp = yearGraph(\"telomerase rna\",2000,2017)\n",
    "# show(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########ASYNCIO version of StateGraph\n",
    "\n",
    "# StateGraph -  used to pull data from pubmed through an api\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "import urllib\n",
    "from urllib.parse import quote\n",
    "#import concurrent.futures\n",
    "#import requests\n",
    "\n",
    "import json\n",
    "from bokeh.sampledata import us_states as usstat\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.models import HoverTool, CustomJS, OpenURL, TapTool, Range1d\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.layouts import layout\n",
    "import pickle\n",
    "\n",
    "\n",
    "#initialize variables\n",
    "dataDir = \"./static/\"\n",
    "moneyFile = \"FundingPerState2016.pkl\"\n",
    "\n",
    "# affiliation = AD\n",
    "searchField = \"[AD]\"\n",
    "us_states = usstat.data.copy()\n",
    "del us_states[\"HI\"]\n",
    "del us_states[\"AK\"]\n",
    "\n",
    "state_xs = [us_states[code][\"lons\"] for code in us_states]\n",
    "state_ys = [us_states[code][\"lats\"] for code in us_states]\n",
    "\n",
    "\n",
    "\n",
    "async def fetchStates(url, session):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "###add states correlating with responses############\n",
    "async def runStates(states, ss, sd, ed):\n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    async with ClientSession() as session:\n",
    "        for state in states:\n",
    "            tu = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\" + state + searchField+\"+AND+\"+ss+\"&mindate=\"+sd+\"&maxdate=\"+ed+\"&usehistory=y&retmode=json\"\n",
    "            task = asyncio.ensure_future(fetchStates(tu, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # you now have all response bodies in this variable\n",
    "        #responses can be converted to json, originally were strings\n",
    "        #print(json.loads(responses[0]))\n",
    "    return responses\n",
    "\n",
    "# gets data from each state while string : ss=searchstring\n",
    "def getStates(ss,sd,ed):\n",
    "    #start threads and create queue of URLs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    states = [us_states[state][\"name\"] for state in us_states]\n",
    "    future = asyncio.ensure_future(runStates(states,ss,sd,ed))\n",
    "    res = loop.run_until_complete(future)\n",
    "    #print(res)\n",
    "    for idx, state in enumerate(us_states):\n",
    "        search_data = json.loads(res[idx])\n",
    "        #webenv = search_data[\"esearchresult\"]['webenv']\n",
    "        total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "        us_states[state][\"count\"] = total_records\n",
    "        #print(total_records)\n",
    "        \n",
    "\n",
    "def stateGraph(si,sd,ed):\n",
    "    ss = quote(si)\n",
    "\n",
    "    ##FOR SEARCHING YOU WILL NEED TO ESCAPE SPACES AND SPECIAL CHARS\n",
    "    getStates(ss,sd,ed)\n",
    "#     for state in us_states:\n",
    "#         print(state)\n",
    "#         print(us_states[state][\"count\"])\n",
    "\n",
    "    # unnormalized to money version\n",
    "    state_counts = [us_states[code][\"count\"] for code in us_states]\n",
    "    state_names = [us_states[code][\"name\"] for code in us_states]\n",
    "    \n",
    "    state_counts_norm = state_counts\n",
    "    state_raw_counts = state_counts\n",
    "    max_state_counts = max(state_counts)\n",
    "    if(max_state_counts > 0):\n",
    "        state_counts = [x / max_state_counts for x in state_counts]\n",
    "    else:\n",
    "        state_counts = [x for x in state_counts]\n",
    "\n",
    "    # normalized to money\n",
    "    fbs = pickle.load(open(dataDir + moneyFile, \"rb\"))\n",
    "    state_counts_norm = [us_states[code][\"count\"] / fbs[us_states[code][\"name\"]] for code in us_states]\n",
    "    max_state_counts_norm = max(state_counts_norm)\n",
    "    if(max_state_counts_norm > 0):\n",
    "        state_counts_norm = [x / max_state_counts_norm for x in state_counts_norm]\n",
    "    else:\n",
    "        state_counts_norm = [x for x in state_counts_norm]\n",
    "        \n",
    "    \n",
    "    stateSource = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=state_xs,\n",
    "                y=state_ys,\n",
    "                state_names = state_names,\n",
    "                state_raw_counts = state_raw_counts,\n",
    "                alphas = state_counts\n",
    "            )\n",
    "        )\n",
    "    stateNormSource = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=state_xs,\n",
    "                y=state_ys,\n",
    "                state_names = state_names,\n",
    "                state_raw_counts = state_raw_counts,\n",
    "                alphas = state_counts_norm\n",
    "            )\n",
    "        )\n",
    "    hoverState = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@state_names</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">Total number of articles:</span>\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@state_raw_counts</span>\n",
    "                <div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "    hoverStateNorm = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@state_names</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">Total Number of Articles:</span>\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@state_raw_counts</span>\n",
    "                <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">Fractional publication rate (norm. by funding):</span>\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@alphas</span>\n",
    "                <div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    TOOLS = 'pan,wheel_zoom,tap,reset'\n",
    "    p = figure(title=\"Publications containing: \" + si,\n",
    "               toolbar_location=\"left\", plot_width=800, plot_height=510, tools=[TOOLS,hoverState], active_scroll='wheel_zoom')\n",
    "    p2 = figure(title=\"Publications containing: \"+si+\" (Normalized by NIH funding)\",\n",
    "                toolbar_location=\"left\", plot_width=800, plot_height=510, tools=[TOOLS,hoverStateNorm], active_scroll='wheel_zoom')\n",
    "    p.xaxis.visible = False\n",
    "    p.xgrid.visible = False\n",
    "    p.yaxis.visible = False\n",
    "    p.ygrid.visible = False\n",
    "    \n",
    "    p2.xaxis.visible = False\n",
    "    p2.xgrid.visible = False\n",
    "    p2.yaxis.visible = False\n",
    "    p2.ygrid.visible = False\n",
    "    \n",
    "    \n",
    "    #p.circle('x', 'y',fill_color='colors', fill_alpha='alphas', size=12, source=source)\n",
    "    p.patches('x', 'y', fill_color=\"#377BA8\", fill_alpha='alphas',\n",
    "              line_color=\"#884444\", line_width=1.5, source=stateSource)\n",
    "\n",
    "    p2.patches('x', 'y', fill_color=\"#377BA8\", fill_alpha='alphas',\n",
    "               line_color=\"#884444\", line_width=1.5, source=stateNormSource)\n",
    "    \n",
    "    #setting up tabs\n",
    "    t1 = Panel(child=p, title= \"Publication Count\")\n",
    "    t2 = Panel(child=p2, title= \"Publication Count (Normalized)*\")\n",
    "    ctabs = Tabs(tabs=[t1,t2],width=800)\n",
    "    lt = layout([[ctabs]])\n",
    "    #show(p)\n",
    "    #show(p2)\n",
    "    return lt\n",
    "\n",
    "# p, p2 = stateGraph(\"prc2\")\n",
    "# show(p)\n",
    "# show(p2)\n",
    "# stateGraph(\"lung\")\n",
    "# stateGraph(\"breast\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###prototyped old code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###plotting subset of \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import HoverTool, CustomJS, OpenURL, TapTool, Range1d\n",
    "from bokeh.models.widgets import TextInput, Button\n",
    "#output_notebook()\n",
    "\n",
    "def calcTSNE(X)\n",
    "    print(\"SVD calc\")\n",
    "    #X = jarr\n",
    "    svd = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "    Xr = svd.fit_transform(X)\n",
    "\n",
    "    print(\"TSNE calc\")\n",
    "    tsn = TSNE(n_components=2, random_state=0)\n",
    "    Y = tsn.fit_transform(Xr);\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# print(\"xmax \"+str(np.amax(Y[:,0])))\n",
    "# print(\"xmin \"+str(np.amin(Y[:,0])))\n",
    "# print(\"ymax \"+str(np.amax(Y[:,1])))\n",
    "# print(\"ymin \"+str(np.amin(Y[:,1])))\n",
    "def similarityGraph(X,ss,ids,titles,authors,journals,dates)\n",
    "    \n",
    "    Y = calcTSNE(X)\n",
    "    \n",
    "    colors = ['blue']*len(ids)\n",
    "    source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=Y[:,0],\n",
    "                y=Y[:,1],\n",
    "                PMID=ids,\n",
    "                titles=titles,\n",
    "                authors=authors,\n",
    "                journals=journals,\n",
    "                dates=dates,\n",
    "                colors=colors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #####max-width IS IMPORTANT FOR PROPER WRAPPING OF TEXT\n",
    "    hover = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@titles</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@authors</span>\n",
    "                <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-style: italic;\">@journals, @dates</span>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 10px;\">PMID</span>\n",
    "                    <span style=\"font-size: 10px; color: #696;\">@PMID</span>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    resetCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var titles = data['titles']\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            data.colors[i]='blue'\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "    textCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var value = cb_obj.get('value')\n",
    "        var titles = data['titles']\n",
    "        var authors = data['authors']\n",
    "        var journals = data['journals']\n",
    "        var words = value.split(\" \")\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            for(j=0; j < words.length; j++){\n",
    "                if (titles[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1) { \n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(authors[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(journals[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else{\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    TOOLS = 'pan,wheel_zoom,tap,reset'\n",
    "    p = figure(plot_width=900, plot_height=400, title=\"'\"+ss+\"' tSNE similarity\", tools=[TOOLS,hover], active_scroll='wheel_zoom')\n",
    "\n",
    "    p.circle('x', 'y',fill_color='colors', size=12, source=source)\n",
    "\n",
    "    #formatting plot\n",
    "    p.xaxis.axis_label = \"Hover to view publication info, Click to open Pubmed link\"\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'\n",
    "    left, right, bottom, top = np.amin(Y[:,0])*1.1, np.amax(Y[:,0])*1.1, np.amin(Y[:,1])*1.1, np.amax(Y[:,1])*1.1\n",
    "    p.x_range=Range1d(left, right)\n",
    "    p.y_range=Range1d(bottom, top)\n",
    "\n",
    "\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pubmed/@PMID\"\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "\n",
    "\n",
    "    word_input = TextInput(title=\"Search for term within dataset (highlight matching pubs)\", callback=textCallback)\n",
    "    reset = Button(label=\"Reset Matching Pubs\", callback=resetCallback, width=150)\n",
    "\n",
    "    layout = column(word_input,reset, p)\n",
    "\n",
    "    show(layout)\n",
    "    #return layout\n",
    "\n",
    "#Plot.scatter(Y[:,0], Y[:,1], 20);\n",
    "#Plot.show();\n",
    "#print(Xr[:5])\n",
    "#print(svd.explained_variance_ratio_.sum()) \n",
    "#Y = tsne(X, 2, 50, 20.0);\n",
    "#Plot.scatter(Y[:,0], Y[:,1], 20, labels);\n",
    "#Plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python 2.7.10, and it requires a working\n",
    "# installation of NumPy. The implementation comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "import numpy as Math\n",
    "import pylab as Plot\n",
    "\n",
    "def Hbeta(D = Math.array([]), beta = 1.0):\n",
    "\t\"\"\"Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution.\"\"\"\n",
    "\n",
    "\t# Compute P-row and corresponding perplexity\n",
    "\tP = Math.exp(-D.copy() * beta);\n",
    "\tsumP = sum(P);\n",
    "\tH = Math.log(sumP) + beta * Math.sum(D * P) / sumP;\n",
    "\tP = P / sumP;\n",
    "\treturn H, P;\n",
    "\n",
    "\n",
    "def x2p(X = Math.array([]), tol = 1e-5, perplexity = 30.0):\n",
    "\t\"\"\"Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity.\"\"\"\n",
    "\n",
    "\t# Initialize some variables\n",
    "\tprint(\"Computing pairwise distances...\")\n",
    "\t(n, d) = X.shape;\n",
    "\tsum_X = Math.sum(Math.square(X), 1);\n",
    "\tD = Math.add(Math.add(-2 * Math.dot(X, X.T), sum_X).T, sum_X);\n",
    "\tP = Math.zeros((n, n));\n",
    "\tbeta = Math.ones((n, 1));\n",
    "\tlogU = Math.log(perplexity);\n",
    "\n",
    "\t# Loop over all datapoints\n",
    "\tfor i in range(n):\n",
    "\n",
    "\t\t# Print progress\n",
    "\t\tif i % 500 == 0:\n",
    "\t\t\tprint(\"Computing P-values for point \", i, \" of \", n, \"...\")\n",
    "\n",
    "\t\t# Compute the Gaussian kernel and entropy for the current precision\n",
    "\t\tbetamin = -Math.inf;\n",
    "\t\tbetamax =  Math.inf;\n",
    "\t\tDi = D[i, Math.concatenate((Math.r_[0:i], Math.r_[i+1:n]))];\n",
    "\t\t(H, thisP) = Hbeta(Di, beta[i]);\n",
    "\n",
    "\t\t# Evaluate whether the perplexity is within tolerance\n",
    "\t\tHdiff = H - logU;\n",
    "\t\ttries = 0;\n",
    "\t\twhile Math.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "\t\t\t# If not, increase or decrease precision\n",
    "\t\t\tif Hdiff > 0:\n",
    "\t\t\t\tbetamin = beta[i].copy();\n",
    "\t\t\t\tif betamax == Math.inf or betamax == -Math.inf:\n",
    "\t\t\t\t\tbeta[i] = beta[i] * 2;\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbeta[i] = (beta[i] + betamax) / 2;\n",
    "\t\t\telse:\n",
    "\t\t\t\tbetamax = beta[i].copy();\n",
    "\t\t\t\tif betamin == Math.inf or betamin == -Math.inf:\n",
    "\t\t\t\t\tbeta[i] = beta[i] / 2;\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbeta[i] = (beta[i] + betamin) / 2;\n",
    "\n",
    "\t\t\t# Recompute the values\n",
    "\t\t\t(H, thisP) = Hbeta(Di, beta[i]);\n",
    "\t\t\tHdiff = H - logU;\n",
    "\t\t\ttries = tries + 1;\n",
    "\n",
    "\t\t# Set the final row of P\n",
    "\t\tP[i, Math.concatenate((Math.r_[0:i], Math.r_[i+1:n]))] = thisP;\n",
    "\n",
    "\t# Return final P-matrix\n",
    "\tprint(\"Mean value of sigma: \", Math.mean(Math.sqrt(1 / beta)));\n",
    "\treturn P;\n",
    "\n",
    "\n",
    "def pca(X = Math.array([]), no_dims = 50):\n",
    "\t\"\"\"Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions.\"\"\"\n",
    "\n",
    "\tprint(\"Preprocessing the data using PCA...\")\n",
    "\t(n, d) = X.shape;\n",
    "\tX = X - Math.tile(Math.mean(X, 0), (n, 1));\n",
    "\t(l, M) = Math.linalg.eig(Math.dot(X.T, X));\n",
    "\tY = Math.dot(X, M[:,0:no_dims]);\n",
    "\treturn Y;\n",
    "\n",
    "\n",
    "def tsne(X = Math.array([]), no_dims = 2, initial_dims = 50, perplexity = 30.0):\n",
    "\t\"\"\"Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions.\n",
    "\tThe syntaxis of the function is Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\"\"\"\n",
    "\n",
    "\t# Check inputs\n",
    "\tif isinstance(no_dims, float):\n",
    "\t\tprint(\"Error: array X should have type float.\");\n",
    "\t\treturn -1;\n",
    "\tif round(no_dims) != no_dims:\n",
    "\t\tprint(\"Error: number of dimensions should be an integer.\");\n",
    "\t\treturn -1;\n",
    "\n",
    "\t# Initialize variables\n",
    "\tX = pca(X, initial_dims).real;\n",
    "\t(n, d) = X.shape;\n",
    "\tmax_iter = 1000;\n",
    "\tinitial_momentum = 0.5;\n",
    "\tfinal_momentum = 0.8;\n",
    "\teta = 500;\n",
    "\tmin_gain = 0.01;\n",
    "\tY = Math.random.randn(n, no_dims);\n",
    "\tdY = Math.zeros((n, no_dims));\n",
    "\tiY = Math.zeros((n, no_dims));\n",
    "\tgains = Math.ones((n, no_dims));\n",
    "\n",
    "\t# Compute P-values\n",
    "\tP = x2p(X, 1e-5, perplexity);\n",
    "\tP = P + Math.transpose(P);\n",
    "\tP = P / Math.sum(P);\n",
    "\tP = P * 4;\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "\tP = Math.maximum(P, 1e-12);\n",
    "\n",
    "\t# Run iterations\n",
    "\tfor iter in range(max_iter):\n",
    "\n",
    "\t\t# Compute pairwise affinities\n",
    "\t\tsum_Y = Math.sum(Math.square(Y), 1);\n",
    "\t\tnum = 1 / (1 + Math.add(Math.add(-2 * Math.dot(Y, Y.T), sum_Y).T, sum_Y));\n",
    "\t\tnum[range(n), range(n)] = 0;\n",
    "\t\tQ = num / Math.sum(num);\n",
    "\t\tQ = Math.maximum(Q, 1e-12);\n",
    "\n",
    "\t\t# Compute gradient\n",
    "\t\tPQ = P - Q;\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tdY[i,:] = Math.sum(Math.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (Y[i,:] - Y), 0);\n",
    "\n",
    "\t\t# Perform the update\n",
    "\t\tif iter < 20:\n",
    "\t\t\tmomentum = initial_momentum\n",
    "\t\telse:\n",
    "\t\t\tmomentum = final_momentum\n",
    "\t\tgains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0));\n",
    "\t\tgains[gains < min_gain] = min_gain;\n",
    "\t\tiY = momentum * iY - eta * (gains * dY);\n",
    "\t\tY = Y + iY;\n",
    "\t\tY = Y - Math.tile(Math.mean(Y, 0), (n, 1));\n",
    "\n",
    "\t\t# Compute current value of cost function\n",
    "\t\tif (iter + 1) % 10 == 0:\n",
    "\t\t\tC = Math.sum(P * Math.log(P / Q));\n",
    "\t\t\tprint(\"Iteration \", (iter + 1), \": error is \", C)\n",
    "\n",
    "\t\t# Stop lying about P-values\n",
    "\t\tif iter == 100:\n",
    "\t\t\tP = P / 4;\n",
    "\n",
    "\t# Return solution\n",
    "\treturn Y;\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
    "    print(\"Running example on 2,500 MNIST digits...\")\n",
    "    X = Math.loadtxt(\"mnist2500_X.txt\");\n",
    "    #print(X.shape)\n",
    "    labels = Math.loadtxt(\"mnist2500_labels.txt\");\n",
    "    Y = tsne(X, 2, 50, 20.0);\n",
    "    Plot.scatter(Y[:,0], Y[:,1], 20, labels);\n",
    "    Plot.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######Get state data based on Json search, prototype code, real implementation above\n",
    "\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def getPubData(si):\n",
    "    ss = quote(si)\n",
    "    y1 = 2016\n",
    "    y2 = 2017\n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+ss+\"&mindate=\"+str(y1)+\"/01/01&maxdate=\"+str(y2)+\"/12/31&usehistory=y&retmode=json\"\n",
    "    search_r = requests.post(search_url)\n",
    "    search_data = search_r.json()\n",
    "    webenv = search_data[\"esearchresult\"]['webenv']\n",
    "    total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&retmax=9999&query_key=1&webenv=\"+webenv\n",
    "\n",
    "    #must grab data in chunks, limited to 10000 per query\n",
    "    allpubs = \"\"\n",
    "    for i in range(0, total_records, 10000):\n",
    "        this_fetch = fetch_url+\"&retstart=\"+str(i)\n",
    "        print(\"Getting this URL: \"+this_fetch)\n",
    "        fetch_r = requests.post(this_fetch)\n",
    "        allpubs += fetch_r.text\n",
    "    print(type(fetch_r))\n",
    "    f = open(\"pubmedsearchdata.xml\", 'w')\n",
    "    f.write(allpubs)\n",
    "    f.close()\n",
    "    print(\"Number of records found :\"+str(total_records))\n",
    "    \n",
    "getPubData(\"cas9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####ASYNCIO CountryGraph\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.sampledata.sample_geojson import geojson\n",
    "from bokeh.models import Range1d, GeoJSONDataSource\n",
    "import pickle\n",
    "import json\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "with open('./static/countries.json') as c:\n",
    "        countriesjson = (c.read())\n",
    "        \n",
    "countriesgj = GeoJSONDataSource(geojson=countriesjson)\n",
    "\n",
    "p = figure(width = 1000, height=500, title='World Countries', x_axis_label='Longitude', y_axis_label='Latitude')\n",
    "\n",
    "p.patches(xs= 'xs', ys='ys', source=countriesgj, fill_color=\"#FFFFFF\", line_color=\"#333333\", line_width=1.5)\n",
    "    \n",
    "    \n",
    "p.x_range = Range1d(start = -180, end = 180)\n",
    "p.y_range = Range1d(start = -90, end = 90)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Get country data and save to pandas\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "countries = requests.get('https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json').json()\n",
    "\n",
    "\n",
    "countryObject = {}\n",
    "for country in countries['features']:\n",
    "    countryObject[country['properties']['name']] = {\n",
    "        'x': [x[0] for x in country['geometry']['coordinates'][0]],\n",
    "        'y': [x[1] for x in country['geometry']['coordinates'][0]],\n",
    "    }\n",
    "countriesjson = open(\"countriesdf.pkl\", 'w')\n",
    "\n",
    "\n",
    "countrydf = pd.DataFrame(countryObject)\n",
    "countryout = open(\"countriesdf.pkl\", 'wb')\n",
    "pickle.dump(countrydf, countryout)\n",
    "countryout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########Javascript bokeh update example\n",
    "\n",
    "from bokeh.io import vform\n",
    "from bokeh.models import HoverTool, CustomJS\n",
    "from bokeh.models.widgets import TextInput\n",
    "from bokeh.plotting import output_file, figure, show, ColumnDataSource\n",
    "\n",
    "output_file(\"plot.html\")\n",
    "\n",
    "words = [\"werner\", \"herbert\", \"klaus\"]\n",
    "x, y = [1,2,3], [1,2,3]\n",
    "color = ['green', 'blue', 'red']\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=x, y=y, words=words, color=color))\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@words\")])\n",
    "\n",
    "p = figure(plot_height=600, plot_width=800, title=\"word2vec\", tools=[hover])\n",
    "p.circle('x','y', radius=0.1, fill_color='color', source=source, line_color=None)\n",
    "\n",
    "callback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "    var data = source.get('data')\n",
    "    var value = cb_obj.get('value')\n",
    "    var words = data['words']\n",
    "    for (i=0; i < words.length; i++) {\n",
    "        if ( words[i]==value ) { data.color[i]='yellow' }\n",
    "    }\n",
    "    source.trigger('change')\n",
    "\"\"\")\n",
    "\n",
    "word_input = TextInput(value=\"word\", title=\"Point out a word\", callback=callback)\n",
    "\n",
    "layout = vform(word_input, p)\n",
    "\n",
    "show(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
