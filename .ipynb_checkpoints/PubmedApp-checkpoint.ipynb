{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Main page: PubmedApp\n",
    "\n",
    "from flask import Flask, render_template, request, redirect\n",
    "from bokeh.embed import components\n",
    "#import PubDatePlotting as pdp\n",
    "#import StateGraph as sg\n",
    "\n",
    "# output_notebook()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.vars = {}\n",
    "\n",
    "#setting up navigation info\n",
    "\n",
    "app.nav_id = {'counts':'counts','geo':'geo','auth_inst':'auth_inst'}\n",
    "app.nav_name = {'counts':'Raw Counts','geo':'Geographic','auth_inst':'Authors & Institutions'}\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    # nquestions=app_lulu.nquestions\n",
    "    if request.method == 'GET':\n",
    "        return render_template('pubsearch.html')\n",
    "    else:\n",
    "        \n",
    "        del app.vars\n",
    "\n",
    "#         app.vars['cancertype'] = request.form['cancertype']\n",
    "\n",
    "#         p1 = pdp.pubByDate(app.vars['cancertype'], False)\n",
    "#         p2 = pdp.pubByDate(app.vars['cancertype'], True)\n",
    "#         p3, p4 = stateGraph(app.vars['cancertype'])\n",
    "#         plots = {'p1': p1, 'p2': p2, 'p3': p3, 'p4': p4}\n",
    "#         script, div = components(plots)\n",
    "\n",
    "        #return render_template('pubsearch.html', script=script, div=div, ctype=app.vars['cancertype'])\n",
    "        return render_template('pubsearch.html')\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/search', methods=['GET', 'POST'])\n",
    "def searchView():\n",
    "    if request.method == 'GET':\n",
    "        app.vars = {}\n",
    "        return render_template('pubsearch.html')\n",
    "    else:\n",
    "        app.vars = {}\n",
    "        app.vars['searchStr'] = request.form['searchterm']\n",
    "        return countsView()\n",
    "\n",
    "\n",
    "        \n",
    "@app.route('/counts', methods=['POST'])\n",
    "def countsView():\n",
    "    app.curpage = \"counts\"\n",
    "    if 'countsData' not in app.vars:\n",
    "        \n",
    "        app.vars['countsData'] = True\n",
    "        \n",
    "#         while True:\n",
    "#             try:\n",
    "#                 app.yearplot = yearGraph(app.vars['searchStr'])\n",
    "#                 break\n",
    "#             except:\n",
    "#                 print(\"Error getting year data\")\n",
    "        app.yearplot = yearGraph(app.vars['searchStr'])\n",
    "        plots = {'yearplot': app.yearplot}\n",
    "        script, div = components(plots)\n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        plots = {'yearplot': app.yearplot}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "@app.route('/geo', methods=['POST'])\n",
    "def geoView():\n",
    "    app.curpage = \"geo\"\n",
    "    if 'geoData' not in app.vars:\n",
    "        app.vars['geoData'] = True\n",
    "        \n",
    "        #connect to Pubmed and try to get data, if fail restart\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 app.stateplot, app.stateplotnorm = stateGraph(app.vars['searchStr'])\n",
    "#                 break\n",
    "#             except:\n",
    "#                 print(\"Error getting state data\")\n",
    "                \n",
    "        app.stateplot, app.stateplotnorm = stateGraph(app.vars['searchStr'])\n",
    "        plots = {'stateplot': app.stateplot, 'stateplotnorm': app.stateplotnorm}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        \n",
    "        plots = {'stateplot': app.stateplot, 'stateplotnorm': app.stateplotnorm}\n",
    "        script, div = components(plots)\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], script=script, div=div, curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "@app.route('/auth_inst', methods=['POST'])\n",
    "def auth_instView():\n",
    "    app.curpage = \"auth_inst\"\n",
    "    if 'auth_instData' not in app.vars:\n",
    "        app.vars['auth_instData'] = True\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "    else:\n",
    "        \n",
    "        ######render\n",
    "        return render_template('pubview.html', searchstring=app.vars['searchStr'], curpage=app.curpage, nav_id=app.nav_id, nav_name=app.nav_name)\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=33507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"a5355671-c32b-4823-9ad0-b4a837eec7bd\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"a5355671-c32b-4823-9ad0-b4a837eec7bd\");\n",
       "      el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"a5355671-c32b-4823-9ad0-b4a837eec7bd\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'a5355671-c32b-4823-9ad0-b4a837eec7bd' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"a5355671-c32b-4823-9ad0-b4a837eec7bd\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"a5355671-c32b-4823-9ad0-b4a837eec7bd\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'urllib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-320c00862ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0mjaccardSimilarityGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meyear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;31m# print(jarr[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-320c00862ff3>\u001b[0m in \u001b[0;36mjaccardSimilarityGraph\u001b[0;34m(si, sy, ey)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjaccardSimilarityGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Searching for PMIDs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPMIDsFromSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urllib' is not defined"
     ]
    }
   ],
   "source": [
    "########Similarity plotting of search\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import HoverTool, CustomJS, OpenURL, TapTool, Range1d\n",
    "from bokeh.models.widgets import TextInput, Button\n",
    "output_notebook()\n",
    "\n",
    "#Articles that a given article cites\n",
    "#https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&id=24923681\n",
    "#add to end of elink post: &query_key=<key>&WebEnv=<webenv string>\n",
    "#Parks Stone Pubmed ID 24923681\n",
    "\n",
    "# UID list. Either a single UID or a comma-delimited list of UIDs may be provided.\n",
    "# All of the UIDs must be from the database specified by dbfrom. \n",
    "# There is no set maximum for the number of UIDs that can be passed to ELink, \n",
    "# but if more than about 200 UIDs are to be provided, the request should be made using the HTTP POST method.\n",
    "\n",
    "ss = \"pediatric glioblastoma\"\n",
    "syear = \"1990\"\n",
    "eyear = \"2017\"\n",
    "\n",
    "#similarity score for comparing sets, ie cited articles\n",
    "#see http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/\n",
    "def jaccard(x,y):\n",
    "\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "def returnJaccard(ids,cids):\n",
    "    jarr = np.empty([len(ids),len(ids)])\n",
    "    for ix,i in enumerate(ids):\n",
    "        for jx,j in enumerate(ids):\n",
    "            if(i>j):\n",
    "                jc = jaccard(cids[ix],cids[jx])\n",
    "                jarr[ix][jx] = jc\n",
    "                jarr[jx][ix] = jc\n",
    "    return jarr\n",
    "\n",
    "### Get ids (PMID) for papers in search\n",
    "def PMIDsFromSearch(s,sy,ey):\n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+s+\"&mindate=\"+sy+\"/01/01&maxdate=\"+ey+\"/12/31&usehistory=y&retmode=json\"\n",
    "    search_r = requests.post(search_url)\n",
    "    search_data = search_r.json()\n",
    "    web_env = search_data['esearchresult']['webenv']\n",
    "    query_key = search_data[\"esearchresult\"]['querykey']\n",
    "    this_fetch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&query_key=\"+query_key+\"&WebEnv=\"+web_env\n",
    "    #this_fetch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&query_key=\"+query_key+\"&WebEnv=\"+web_env+\"&cmd=neighbor_score\"\n",
    "    #this_fetch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pubmed&id=24923681&cmd=neighbor_score\"\n",
    "    fetch_r = requests.post(this_fetch)\n",
    "    #print(fetch_r.text)\n",
    "\n",
    "    return ET.fromstring(fetch_r.text)\n",
    "\n",
    "\n",
    "########CHANGE CODE BELOW need to grab all ids summary to get the title, authors, pubdate for each\n",
    "def getPMIDInfo(ids):\n",
    "    #use full journal name\n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "    journals = []\n",
    "    \n",
    "    #https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&version=2.0&id=27656642,24923681\n",
    "    info_base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&version=2.0&id=\"\n",
    "    info_post = info_base\n",
    "    \n",
    "    #loop over all PMIDs\n",
    "    for tid in ids:\n",
    "        if((len(info_post)+len(str(tid))+4)>2083):\n",
    "            \n",
    "            #print(\"fetching\")\n",
    "            info_post = info_post[:-1]\n",
    "            info_fetch = requests.post(info_post)\n",
    "            info_post = info_base+str(tid)+\",\"\n",
    "            croot = ET.fromstring(info_fetch.text)\n",
    "\n",
    "            #get data from croot\n",
    "            for doc in croot[0].iter('DocumentSummary'):\n",
    "                titles.append(doc.find('Title').text)\n",
    "                dates.append(doc.find('PubDate').text)\n",
    "                journals.append(doc.find('FullJournalName').text)\n",
    "                aus = \"\"\n",
    "                for auth in doc.find('Authors').iter('Author'):\n",
    "                    aus += auth.find('Name').text+\", \"\n",
    "                authors.append(aus[:-2])\n",
    "                \n",
    "        else:\n",
    "            info_post += str(tid)+\",\"\n",
    "\n",
    "    #for grabbing last set of Links\n",
    "    \n",
    "    info_post = info_post[:-1]\n",
    "    info_fetch = requests.post(info_post)\n",
    "    croot = ET.fromstring(info_fetch.text)\n",
    "\n",
    "    for doc in croot[0].iter('DocumentSummary'):\n",
    "        titles.append(doc.find('Title').text)\n",
    "        dates.append(doc.find('PubDate').text)\n",
    "        journals.append(doc.find('FullJournalName').text)\n",
    "        aus = \"\"\n",
    "        for auth in doc.find('Authors').iter('Author'):\n",
    "            aus += auth.find('Name').text+\", \"\n",
    "        authors.append(aus[:-2])\n",
    "    return titles, dates, authors, journals\n",
    "\n",
    "#ids = np.empty([len(root[0][1].getchildren()),1])\n",
    "def getCitedFromPMIDXML(r):\n",
    "    ids = []\n",
    "    cids = []\n",
    "\n",
    "    cited_xml = \"\"\n",
    "    cited_base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs\"\n",
    "    cited_post = cited_base\n",
    "\n",
    "    #loop over all articles and grab set cited papers (see cited_rosetta for paper structure)\n",
    "    for tid in r[0][1].iter('Id'):\n",
    "        if((len(cited_post)+len(tid.text)+4)>2083):\n",
    "            #print(\"fetching\")\n",
    "            cited_fetch = requests.post(cited_post)\n",
    "            cited_post = cited_base+\"&id=\"+tid.text\n",
    "            croot = ET.fromstring(cited_fetch.text)\n",
    "\n",
    "            for linkset in croot.iter('LinkSet'):\n",
    "                tlinks = []\n",
    "                ttid = linkset.find('IdList')[0]\n",
    "                if(len(list(linkset))>2):\n",
    "                    for link in linkset.find('LinkSetDb').iter('Link'):\n",
    "                        tlid = link[0]\n",
    "                        tlinks.append(tlid.text)\n",
    "                    cids.append(tlinks)\n",
    "                    ids.append(int(ttid.text))\n",
    "        else:\n",
    "            cited_post += \"&id=\"+tid.text\n",
    "\n",
    "    #for grabbing last set of Links\n",
    "    cited_fetch = requests.post(cited_post)\n",
    "    cited_post = cited_base+\"&id=\"+tid.text\n",
    "    croot = ET.fromstring(cited_fetch.text)\n",
    "\n",
    "    for linkset in croot.iter('LinkSet'):\n",
    "        tlinks = []\n",
    "        ttid = linkset.find('IdList')[0]\n",
    "        if(len(list(linkset))>2):\n",
    "            for link in linkset.find('LinkSetDb').iter('Link'):\n",
    "                tlid = link[0]\n",
    "                tlinks.append(int(tlid.text))\n",
    "            cids.append(tlinks)\n",
    "            ids.append(int(ttid.text))\n",
    "    return ids, cids\n",
    "\n",
    "\n",
    "def calcTSNE(X):\n",
    "    print(\"SVD calc\")\n",
    "    #X = jarr\n",
    "    svd = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "    Xr = svd.fit_transform(X)\n",
    "\n",
    "    print(\"TSNE calc\")\n",
    "    tsn = TSNE(n_components=2, random_state=0)\n",
    "    Y = tsn.fit_transform(Xr);\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# print(\"xmax \"+str(np.amax(Y[:,0])))\n",
    "# print(\"xmin \"+str(np.amin(Y[:,0])))\n",
    "# print(\"ymax \"+str(np.amax(Y[:,1])))\n",
    "# print(\"ymin \"+str(np.amin(Y[:,1])))\n",
    "def jaccardSimilarityGraph(si, sy, ey):\n",
    "    \n",
    "    ss = quote(si)\n",
    "    print(\"Searching for PMIDs\")\n",
    "    root = PMIDsFromSearch(ss, sy, ey)\n",
    "    print(\"Getting Cited PMIDs\")\n",
    "    ids, cids = getCitedFromPMIDXML(root)\n",
    "    print(str(len(ids))+\" Articles Found\")\n",
    "    print(\"Getting Info of PMIDs\")\n",
    "    titles, dates, authors, journals = getPMIDInfo(ids)\n",
    "    print(\"Performing Jaccard comparison\")\n",
    "    jarr = returnJaccard(ids, cids)\n",
    "    print(\"min jaccard:\"+str(np.amin(jarr)))\n",
    "    Y = calcTSNE(jarr)\n",
    "    \n",
    "    colors = ['blue']*len(ids)\n",
    "    source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=Y[:,0],\n",
    "                y=Y[:,1],\n",
    "                PMID=ids,\n",
    "                titles=titles,\n",
    "                authors=authors,\n",
    "                journals=journals,\n",
    "                dates=dates,\n",
    "                colors=colors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #####max-width IS IMPORTANT FOR PROPER WRAPPING OF TEXT\n",
    "    hover = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@titles</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@authors</span>\n",
    "                <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-style: italic;\">@journals, @dates</span>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 10px;\">PMID</span>\n",
    "                    <span style=\"font-size: 10px; color: #696;\">@PMID</span>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    resetCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var titles = data['titles']\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            data.colors[i]='blue'\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "    textCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var value = cb_obj.get('value')\n",
    "        var titles = data['titles']\n",
    "        var authors = data['authors']\n",
    "        var journals = data['journals']\n",
    "        var words = value.split(\" \")\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            for(j=0; j < words.length; j++){\n",
    "                if (titles[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1) { \n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(authors[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(journals[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else{\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    TOOLS = 'pan,wheel_zoom,tap,reset'\n",
    "    p = figure(plot_width=900, plot_height=400, title=\"'\"+si+\"' tSNE similarity\", tools=[TOOLS,hover], active_scroll='wheel_zoom')\n",
    "\n",
    "    p.circle('x', 'y',fill_color='colors', size=12, source=source)\n",
    "\n",
    "    #formatting plot\n",
    "    p.xaxis.axis_label = \"Hover to view publication info, Click to open Pubmed link\"\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'\n",
    "    left, right, bottom, top = np.amin(Y[:,0])*1.1, np.amax(Y[:,0])*1.1, np.amin(Y[:,1])*1.1, np.amax(Y[:,1])*1.1\n",
    "    p.x_range=Range1d(left, right)\n",
    "    p.y_range=Range1d(bottom, top)\n",
    "\n",
    "\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pubmed/@PMID\"\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "\n",
    "\n",
    "    word_input = TextInput(title=\"Search for term within dataset (highlight matching pubs)\", callback=textCallback)\n",
    "    reset = Button(label=\"Reset Matching Pubs\", callback=resetCallback, width=150)\n",
    "\n",
    "    layout = column(word_input,reset, p)\n",
    "\n",
    "    show(layout)\n",
    "\n",
    "    \n",
    "jaccardSimilarityGraph(ss,syear,eyear)\n",
    "\n",
    "# print(jarr[:5])\n",
    "# print(ids[:5])\n",
    "# print(cids[:5])\n",
    "# print(titles[:5])\n",
    "# print(authors[:5])\n",
    "# print(dates[:5])\n",
    "# print(journals[:5])\n",
    "\n",
    "\n",
    "# cited_fetch = requests.post(cited_post)\n",
    "# cited_xml += cited_fetch.text\n",
    "# f = open(\"cited_\"+ss+\".xml\", 'w')\n",
    "# f.write(cited_xml)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import HoverTool, CustomJS, OpenURL, TapTool, Range1d\n",
    "from bokeh.models.widgets import TextInput, Button\n",
    "#output_notebook()\n",
    "\n",
    "def calcTSNE(X)\n",
    "    print(\"SVD calc\")\n",
    "    #X = jarr\n",
    "    svd = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "    Xr = svd.fit_transform(X)\n",
    "\n",
    "    print(\"TSNE calc\")\n",
    "    tsn = TSNE(n_components=2, random_state=0)\n",
    "    Y = tsn.fit_transform(Xr);\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# print(\"xmax \"+str(np.amax(Y[:,0])))\n",
    "# print(\"xmin \"+str(np.amin(Y[:,0])))\n",
    "# print(\"ymax \"+str(np.amax(Y[:,1])))\n",
    "# print(\"ymin \"+str(np.amin(Y[:,1])))\n",
    "def similarityGraph(X,ss,ids,titles,authors,journals,dates)\n",
    "    \n",
    "    Y = calcTSNE(X)\n",
    "    \n",
    "    colors = ['blue']*len(ids)\n",
    "    source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=Y[:,0],\n",
    "                y=Y[:,1],\n",
    "                PMID=ids,\n",
    "                titles=titles,\n",
    "                authors=authors,\n",
    "                journals=journals,\n",
    "                dates=dates,\n",
    "                colors=colors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #####max-width IS IMPORTANT FOR PROPER WRAPPING OF TEXT\n",
    "    hover = HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "            <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-weight: bold;\">@titles</span>\n",
    "                </div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; color: #966;\">@authors</span>\n",
    "                <div>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 12px; font-style: italic;\">@journals, @dates</span>\n",
    "                <div style=\"max-width: 400px;\">\n",
    "                    <span style=\"font-size: 10px;\">PMID</span>\n",
    "                    <span style=\"font-size: 10px; color: #696;\">@PMID</span>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    resetCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var titles = data['titles']\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            data.colors[i]='blue'\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "    textCallback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "        var data = source.get('data')\n",
    "        var value = cb_obj.get('value')\n",
    "        var titles = data['titles']\n",
    "        var authors = data['authors']\n",
    "        var journals = data['journals']\n",
    "        var words = value.split(\" \")\n",
    "        for (i=0; i < titles.length; i++) {\n",
    "            for(j=0; j < words.length; j++){\n",
    "                if (titles[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1) { \n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(authors[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else if(journals[i].toLowerCase().indexOf(words[j].toLowerCase()) !== -1){\n",
    "                    if(j == words.length-1){\n",
    "                        data.colors[i]='orange' \n",
    "                    }\n",
    "                }else{\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        source.trigger('change')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    TOOLS = 'pan,wheel_zoom,tap,reset'\n",
    "    p = figure(plot_width=900, plot_height=400, title=\"'\"+ss+\"' tSNE similarity\", tools=[TOOLS,hover], active_scroll='wheel_zoom')\n",
    "\n",
    "    p.circle('x', 'y',fill_color='colors', size=12, source=source)\n",
    "\n",
    "    #formatting plot\n",
    "    p.xaxis.axis_label = \"Hover to view publication info, Click to open Pubmed link\"\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'\n",
    "    left, right, bottom, top = np.amin(Y[:,0])*1.1, np.amax(Y[:,0])*1.1, np.amin(Y[:,1])*1.1, np.amax(Y[:,1])*1.1\n",
    "    p.x_range=Range1d(left, right)\n",
    "    p.y_range=Range1d(bottom, top)\n",
    "\n",
    "\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pubmed/@PMID\"\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "\n",
    "\n",
    "    word_input = TextInput(title=\"Search for term within dataset (highlight matching pubs)\", callback=textCallback)\n",
    "    reset = Button(label=\"Reset Matching Pubs\", callback=resetCallback, width=150)\n",
    "\n",
    "    layout = column(word_input,reset, p)\n",
    "\n",
    "    show(layout)\n",
    "    #return layout\n",
    "\n",
    "#Plot.scatter(Y[:,0], Y[:,1], 20);\n",
    "#Plot.show();\n",
    "#print(Xr[:5])\n",
    "#print(svd.explained_variance_ratio_.sum()) \n",
    "#Y = tsne(X, 2, 50, 20.0);\n",
    "#Plot.scatter(Y[:,0], Y[:,1], 20, labels);\n",
    "#Plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(jaccard(cids[ids.index(27895053)],cids[ids.index(27180908)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############ASYCIO PubByDate\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "import urllib\n",
    "from urllib.parse import quote\n",
    "#import concurrent.futures\n",
    "#import requests\n",
    "\n",
    "import json\n",
    "from bokeh.sampledata import us_states\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "\n",
    "years = list(range(1975, 2017))\n",
    "\n",
    "async def fetchYears(url, session):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "###add years correlating with responses############\n",
    "async def runYears(yrs, ss):\n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    async with ClientSession() as session:\n",
    "        for y in yrs:\n",
    "            tu = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+ss+\"&mindate=\"+str(y)+\"/01/01&maxdate=\"+str(y)+\"/12/31&usehistory=y&retmode=json\"\n",
    "            task = asyncio.ensure_future(fetchYears(tu, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # you now have all response bodies in this variable\n",
    "        #responses can be converted to json, originally were strings\n",
    "        #print(json.loads(responses[0]))\n",
    "    return responses\n",
    "\n",
    "# gets data from each state while string : ss=searchstring\n",
    "def getYears(ss):\n",
    "    \n",
    "    yearcounts = []\n",
    "    #start threads and create queue of URLs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(runYears(years,ss))\n",
    "    res = loop.run_until_complete(future)\n",
    "    #print(res)\n",
    "    for idx, y in enumerate(years):\n",
    "        search_data = json.loads(res[idx])\n",
    "        #webenv = search_data[\"esearchresult\"]['webenv']\n",
    "        total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "        yearcounts.append(total_records) \n",
    "        #print(total_records)\n",
    "    return yearcounts\n",
    "        \n",
    "\n",
    "def yearGraph(si):\n",
    "    ss = quote(si)\n",
    "    p = figure(title=\"Articles containing: \"+si, plot_width=400, plot_height=400, x_axis_label=\"Year\", y_axis_label=\"Total Articles\")\n",
    "\n",
    "    ##FOR SEARCHING YOU WILL NEED TO ESCAPE SPACES AND SPECIAL CHARS\n",
    "    yearcounts = getYears(ss)\n",
    "#     for state in us_states:\n",
    "#         print(state)\n",
    "#         print(us_states[state][\"count\"])\n",
    "\n",
    "    # unnormalized to money version\n",
    "    p.line(years, yearcounts, color='blue')\n",
    "\n",
    "    #show(p)\n",
    "    return p\n",
    "\n",
    "#p, p2 = stateGraph(\"prostate\")\n",
    "#show(p)\n",
    "#show(p2)\n",
    "# stateGraph(\"lung\")\n",
    "# stateGraph(\"breast\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########ASYNCIO version of StateGraph\n",
    "\n",
    "# StateGraph -  used to pull data from pubmed through an api\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "import urllib\n",
    "from urllib.parse import quote\n",
    "#import concurrent.futures\n",
    "#import requests\n",
    "\n",
    "import json\n",
    "from bokeh.sampledata import us_states\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "import pickle\n",
    "\n",
    "dataDir = \"./static/\"\n",
    "moneyFile = \"FundingPerState2016.pkl\"\n",
    "\n",
    "# affiliation = AD\n",
    "searchField = \"[AD]\"\n",
    "us_states = us_states.data.copy()\n",
    "del us_states[\"HI\"]\n",
    "del us_states[\"AK\"]\n",
    "\n",
    "state_xs = [us_states[code][\"lons\"] for code in us_states]\n",
    "state_ys = [us_states[code][\"lats\"] for code in us_states]\n",
    "\n",
    "async def fetchStates(url, session):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "###add states correlating with responses############\n",
    "async def runStates(states, ss):\n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    async with ClientSession() as session:\n",
    "        for state in states:\n",
    "            tu = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\" + state + searchField+\"+AND+\" +ss+\"&mindate=2012/01/01&maxdate=2016/12/31&usehistory=y&retmode=json\"\n",
    "            task = asyncio.ensure_future(fetchStates(tu, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # you now have all response bodies in this variable\n",
    "        #responses can be converted to json, originally were strings\n",
    "        #print(json.loads(responses[0]))\n",
    "    return responses\n",
    "\n",
    "# gets data from each state while string : ss=searchstring\n",
    "def getStates(ss):\n",
    "    #start threads and create queue of URLs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    states = [us_states[state][\"name\"] for state in us_states]\n",
    "    future = asyncio.ensure_future(runStates(states,ss))\n",
    "    res = loop.run_until_complete(future)\n",
    "    #print(res)\n",
    "    for idx, state in enumerate(us_states):\n",
    "        search_data = json.loads(res[idx])\n",
    "        #webenv = search_data[\"esearchresult\"]['webenv']\n",
    "        total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "        us_states[state][\"count\"] = total_records\n",
    "        #print(total_records)\n",
    "        \n",
    "\n",
    "def stateGraph(si):\n",
    "    ss = quote(si)\n",
    "    p = figure(title=\"Publications containing: \" + si,\n",
    "               toolbar_location=\"left\", plot_width=800, plot_height=510)\n",
    "    p2 = figure(title=\"Publications containing: \"+si+\" (Normalized by funding)\",\n",
    "                toolbar_location=\"left\", plot_width=800, plot_height=510)\n",
    "    p.xaxis.visible = False\n",
    "    p.xgrid.visible = False\n",
    "    p.yaxis.visible = False\n",
    "    p.ygrid.visible = False\n",
    "    \n",
    "    p2.xaxis.visible = False\n",
    "    p2.xgrid.visible = False\n",
    "    p2.yaxis.visible = False\n",
    "    p2.ygrid.visible = False\n",
    "\n",
    "    ##FOR SEARCHING YOU WILL NEED TO ESCAPE SPACES AND SPECIAL CHARS\n",
    "    getStates(ss)\n",
    "#     for state in us_states:\n",
    "#         print(state)\n",
    "#         print(us_states[state][\"count\"])\n",
    "\n",
    "    # unnormalized to money version\n",
    "    state_counts = [us_states[code][\"count\"] for code in us_states]\n",
    "    state_counts_norm = state_counts\n",
    "    max_state_counts = max(state_counts)\n",
    "    if(max_state_counts > 0):\n",
    "        state_counts = [x / max_state_counts for x in state_counts]\n",
    "    else:\n",
    "        state_counts = [x for x in state_counts]\n",
    "\n",
    "    # normalized to money\n",
    "    fbs = pickle.load(open(dataDir + moneyFile, \"rb\"))\n",
    "    state_counts_norm = [us_states[code][\"count\"] / fbs[us_states[code][\"name\"]] for code in us_states]\n",
    "    max_state_counts_norm = max(state_counts_norm)\n",
    "    if(max_state_counts_norm > 0):\n",
    "        state_counts_norm = [x / max_state_counts_norm for x in state_counts_norm]\n",
    "    else:\n",
    "        state_counts_norm = [x for x in state_counts_norm]\n",
    "    p.patches(state_xs, state_ys, fill_color=\"#377BA8\", fill_alpha=state_counts,\n",
    "              line_color=\"#884444\", line_width=1.5)\n",
    "\n",
    "    p2.patches(state_xs, state_ys, fill_color=\"#377BA8\", fill_alpha=state_counts_norm,\n",
    "               line_color=\"#884444\", line_width=1.5)\n",
    "\n",
    "    #show(p)\n",
    "    #show(p2)\n",
    "    return p, p2\n",
    "\n",
    "#p, p2 = stateGraph(\"prostate\")\n",
    "#show(p)\n",
    "#show(p2)\n",
    "# stateGraph(\"lung\")\n",
    "# stateGraph(\"breast\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python 2.7.10, and it requires a working\n",
    "# installation of NumPy. The implementation comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "import numpy as Math\n",
    "import pylab as Plot\n",
    "\n",
    "def Hbeta(D = Math.array([]), beta = 1.0):\n",
    "\t\"\"\"Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution.\"\"\"\n",
    "\n",
    "\t# Compute P-row and corresponding perplexity\n",
    "\tP = Math.exp(-D.copy() * beta);\n",
    "\tsumP = sum(P);\n",
    "\tH = Math.log(sumP) + beta * Math.sum(D * P) / sumP;\n",
    "\tP = P / sumP;\n",
    "\treturn H, P;\n",
    "\n",
    "\n",
    "def x2p(X = Math.array([]), tol = 1e-5, perplexity = 30.0):\n",
    "\t\"\"\"Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity.\"\"\"\n",
    "\n",
    "\t# Initialize some variables\n",
    "\tprint(\"Computing pairwise distances...\")\n",
    "\t(n, d) = X.shape;\n",
    "\tsum_X = Math.sum(Math.square(X), 1);\n",
    "\tD = Math.add(Math.add(-2 * Math.dot(X, X.T), sum_X).T, sum_X);\n",
    "\tP = Math.zeros((n, n));\n",
    "\tbeta = Math.ones((n, 1));\n",
    "\tlogU = Math.log(perplexity);\n",
    "\n",
    "\t# Loop over all datapoints\n",
    "\tfor i in range(n):\n",
    "\n",
    "\t\t# Print progress\n",
    "\t\tif i % 500 == 0:\n",
    "\t\t\tprint(\"Computing P-values for point \", i, \" of \", n, \"...\")\n",
    "\n",
    "\t\t# Compute the Gaussian kernel and entropy for the current precision\n",
    "\t\tbetamin = -Math.inf;\n",
    "\t\tbetamax =  Math.inf;\n",
    "\t\tDi = D[i, Math.concatenate((Math.r_[0:i], Math.r_[i+1:n]))];\n",
    "\t\t(H, thisP) = Hbeta(Di, beta[i]);\n",
    "\n",
    "\t\t# Evaluate whether the perplexity is within tolerance\n",
    "\t\tHdiff = H - logU;\n",
    "\t\ttries = 0;\n",
    "\t\twhile Math.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "\t\t\t# If not, increase or decrease precision\n",
    "\t\t\tif Hdiff > 0:\n",
    "\t\t\t\tbetamin = beta[i].copy();\n",
    "\t\t\t\tif betamax == Math.inf or betamax == -Math.inf:\n",
    "\t\t\t\t\tbeta[i] = beta[i] * 2;\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbeta[i] = (beta[i] + betamax) / 2;\n",
    "\t\t\telse:\n",
    "\t\t\t\tbetamax = beta[i].copy();\n",
    "\t\t\t\tif betamin == Math.inf or betamin == -Math.inf:\n",
    "\t\t\t\t\tbeta[i] = beta[i] / 2;\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbeta[i] = (beta[i] + betamin) / 2;\n",
    "\n",
    "\t\t\t# Recompute the values\n",
    "\t\t\t(H, thisP) = Hbeta(Di, beta[i]);\n",
    "\t\t\tHdiff = H - logU;\n",
    "\t\t\ttries = tries + 1;\n",
    "\n",
    "\t\t# Set the final row of P\n",
    "\t\tP[i, Math.concatenate((Math.r_[0:i], Math.r_[i+1:n]))] = thisP;\n",
    "\n",
    "\t# Return final P-matrix\n",
    "\tprint(\"Mean value of sigma: \", Math.mean(Math.sqrt(1 / beta)));\n",
    "\treturn P;\n",
    "\n",
    "\n",
    "def pca(X = Math.array([]), no_dims = 50):\n",
    "\t\"\"\"Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions.\"\"\"\n",
    "\n",
    "\tprint(\"Preprocessing the data using PCA...\")\n",
    "\t(n, d) = X.shape;\n",
    "\tX = X - Math.tile(Math.mean(X, 0), (n, 1));\n",
    "\t(l, M) = Math.linalg.eig(Math.dot(X.T, X));\n",
    "\tY = Math.dot(X, M[:,0:no_dims]);\n",
    "\treturn Y;\n",
    "\n",
    "\n",
    "def tsne(X = Math.array([]), no_dims = 2, initial_dims = 50, perplexity = 30.0):\n",
    "\t\"\"\"Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions.\n",
    "\tThe syntaxis of the function is Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\"\"\"\n",
    "\n",
    "\t# Check inputs\n",
    "\tif isinstance(no_dims, float):\n",
    "\t\tprint(\"Error: array X should have type float.\");\n",
    "\t\treturn -1;\n",
    "\tif round(no_dims) != no_dims:\n",
    "\t\tprint(\"Error: number of dimensions should be an integer.\");\n",
    "\t\treturn -1;\n",
    "\n",
    "\t# Initialize variables\n",
    "\tX = pca(X, initial_dims).real;\n",
    "\t(n, d) = X.shape;\n",
    "\tmax_iter = 1000;\n",
    "\tinitial_momentum = 0.5;\n",
    "\tfinal_momentum = 0.8;\n",
    "\teta = 500;\n",
    "\tmin_gain = 0.01;\n",
    "\tY = Math.random.randn(n, no_dims);\n",
    "\tdY = Math.zeros((n, no_dims));\n",
    "\tiY = Math.zeros((n, no_dims));\n",
    "\tgains = Math.ones((n, no_dims));\n",
    "\n",
    "\t# Compute P-values\n",
    "\tP = x2p(X, 1e-5, perplexity);\n",
    "\tP = P + Math.transpose(P);\n",
    "\tP = P / Math.sum(P);\n",
    "\tP = P * 4;\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "\tP = Math.maximum(P, 1e-12);\n",
    "\n",
    "\t# Run iterations\n",
    "\tfor iter in range(max_iter):\n",
    "\n",
    "\t\t# Compute pairwise affinities\n",
    "\t\tsum_Y = Math.sum(Math.square(Y), 1);\n",
    "\t\tnum = 1 / (1 + Math.add(Math.add(-2 * Math.dot(Y, Y.T), sum_Y).T, sum_Y));\n",
    "\t\tnum[range(n), range(n)] = 0;\n",
    "\t\tQ = num / Math.sum(num);\n",
    "\t\tQ = Math.maximum(Q, 1e-12);\n",
    "\n",
    "\t\t# Compute gradient\n",
    "\t\tPQ = P - Q;\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tdY[i,:] = Math.sum(Math.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (Y[i,:] - Y), 0);\n",
    "\n",
    "\t\t# Perform the update\n",
    "\t\tif iter < 20:\n",
    "\t\t\tmomentum = initial_momentum\n",
    "\t\telse:\n",
    "\t\t\tmomentum = final_momentum\n",
    "\t\tgains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0));\n",
    "\t\tgains[gains < min_gain] = min_gain;\n",
    "\t\tiY = momentum * iY - eta * (gains * dY);\n",
    "\t\tY = Y + iY;\n",
    "\t\tY = Y - Math.tile(Math.mean(Y, 0), (n, 1));\n",
    "\n",
    "\t\t# Compute current value of cost function\n",
    "\t\tif (iter + 1) % 10 == 0:\n",
    "\t\t\tC = Math.sum(P * Math.log(P / Q));\n",
    "\t\t\tprint(\"Iteration \", (iter + 1), \": error is \", C)\n",
    "\n",
    "\t\t# Stop lying about P-values\n",
    "\t\tif iter == 100:\n",
    "\t\t\tP = P / 4;\n",
    "\n",
    "\t# Return solution\n",
    "\treturn Y;\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
    "    print(\"Running example on 2,500 MNIST digits...\")\n",
    "    X = Math.loadtxt(\"mnist2500_X.txt\");\n",
    "    #print(X.shape)\n",
    "    labels = Math.loadtxt(\"mnist2500_labels.txt\");\n",
    "    Y = tsne(X, 2, 50, 20.0);\n",
    "    Plot.scatter(Y[:,0], Y[:,1], 20, labels);\n",
    "    Plot.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######Get state data based on Json search, prototype code, real implementation above\n",
    "\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def getPubData(si):\n",
    "    ss = quote(si)\n",
    "    y1 = 2016\n",
    "    y2 = 2017\n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\"+ss+\"&mindate=\"+str(y1)+\"/01/01&maxdate=\"+str(y2)+\"/12/31&usehistory=y&retmode=json\"\n",
    "    search_r = requests.post(search_url)\n",
    "    search_data = search_r.json()\n",
    "    webenv = search_data[\"esearchresult\"]['webenv']\n",
    "    total_records = int(search_data[\"esearchresult\"]['count'])\n",
    "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&retmax=9999&query_key=1&webenv=\"+webenv\n",
    "\n",
    "    #must grab data in chunks, limited to 10000 per query\n",
    "    allpubs = \"\"\n",
    "    for i in range(0, total_records, 10000):\n",
    "        this_fetch = fetch_url+\"&retstart=\"+str(i)\n",
    "        print(\"Getting this URL: \"+this_fetch)\n",
    "        fetch_r = requests.post(this_fetch)\n",
    "        allpubs += fetch_r.text\n",
    "    print(type(fetch_r))\n",
    "    f = open(\"pubmedsearchdata.xml\", 'w')\n",
    "    f.write(allpubs)\n",
    "    f.close()\n",
    "    print(\"Number of records found :\"+str(total_records))\n",
    "    \n",
    "getPubData(\"cas9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####ASYNCIO CountryGraph\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.sampledata.sample_geojson import geojson\n",
    "from bokeh.models import Range1d, GeoJSONDataSource\n",
    "import pickle\n",
    "import json\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "with open('./static/countries.json') as c:\n",
    "        countriesjson = (c.read())\n",
    "        \n",
    "countriesgj = GeoJSONDataSource(geojson=countriesjson)\n",
    "\n",
    "p = figure(width = 1000, height=500, title='World Countries', x_axis_label='Longitude', y_axis_label='Latitude')\n",
    "\n",
    "p.patches(xs= 'xs', ys='ys', source=countriesgj, fill_color=\"#FFFFFF\", line_color=\"#333333\", line_width=1.5)\n",
    "    \n",
    "    \n",
    "p.x_range = Range1d(start = -180, end = 180)\n",
    "p.y_range = Range1d(start = -90, end = 90)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Get country data and save to pandas\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "countries = requests.get('https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json').json()\n",
    "\n",
    "\n",
    "countryObject = {}\n",
    "for country in countries['features']:\n",
    "    countryObject[country['properties']['name']] = {\n",
    "        'x': [x[0] for x in country['geometry']['coordinates'][0]],\n",
    "        'y': [x[1] for x in country['geometry']['coordinates'][0]],\n",
    "    }\n",
    "countriesjson = open(\"countriesdf.pkl\", 'w')\n",
    "\n",
    "\n",
    "countrydf = pd.DataFrame(countryObject)\n",
    "countryout = open(\"countriesdf.pkl\", 'wb')\n",
    "pickle.dump(countrydf, countryout)\n",
    "countryout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########Javascript bokeh update example\n",
    "\n",
    "from bokeh.io import vform\n",
    "from bokeh.models import HoverTool, CustomJS\n",
    "from bokeh.models.widgets import TextInput\n",
    "from bokeh.plotting import output_file, figure, show, ColumnDataSource\n",
    "\n",
    "output_file(\"plot.html\")\n",
    "\n",
    "words = [\"werner\", \"herbert\", \"klaus\"]\n",
    "x, y = [1,2,3], [1,2,3]\n",
    "color = ['green', 'blue', 'red']\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=x, y=y, words=words, color=color))\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@words\")])\n",
    "\n",
    "p = figure(plot_height=600, plot_width=800, title=\"word2vec\", tools=[hover])\n",
    "p.circle('x','y', radius=0.1, fill_color='color', source=source, line_color=None)\n",
    "\n",
    "callback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "    var data = source.get('data')\n",
    "    var value = cb_obj.get('value')\n",
    "    var words = data['words']\n",
    "    for (i=0; i < words.length; i++) {\n",
    "        if ( words[i]==value ) { data.color[i]='yellow' }\n",
    "    }\n",
    "    source.trigger('change')\n",
    "\"\"\")\n",
    "\n",
    "word_input = TextInput(value=\"word\", title=\"Point out a word\", callback=callback)\n",
    "\n",
    "layout = vform(word_input, p)\n",
    "\n",
    "show(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
